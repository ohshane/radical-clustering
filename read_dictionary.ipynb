{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDS path\t: /home/shaneoh/workspace/radical-clustering/data/ids_mod.tsv\n",
      "Dictionary path\t: /home/shaneoh/workspace/radical-clustering/data/dictionary.txt\n",
      "atoms 469\n",
      "nonatoms_decomposed 88480\n",
      "decomposed 88949\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "root_dir = Path(os.path.abspath(\"__file__\")).parent\n",
    "dataset_dir = root_dir / \"data\"\n",
    "ids_file_dir = dataset_dir / \"ids_mod.tsv\"\n",
    "dic_dir = dataset_dir / \"dictionary.txt\"\n",
    "pickle_dir = root_dir / \"pickle\"\n",
    "\n",
    "print(f\"IDS path\\t: {ids_file_dir}\")\n",
    "print(f\"Dictionary path\\t: {dic_dir}\")\n",
    "\n",
    "with open(pickle_dir / 'atoms.pickle', 'rb') as f:\n",
    "    atoms = pickle.load(f)\n",
    "\n",
    "with open(pickle_dir / 'nonatoms_decomposed.pickle', 'rb') as f:\n",
    "    nonatoms_decomposed = pickle.load(f)\n",
    "\n",
    "print(f'atoms {len(atoms)}')\n",
    "print(f'nonatoms_decomposed {len(nonatoms_decomposed)}')\n",
    "\n",
    "atoms_dict = dict(zip(atoms[:,0], atoms[:,2]))\n",
    "nonatoms_decomposed_dict = dict(zip(nonatoms_decomposed[:,0], nonatoms_decomposed[:,2]))\n",
    "decomposed_dict = atoms_dict.copy()\n",
    "decomposed_dict.update(nonatoms_decomposed_dict)\n",
    "\n",
    "print(f'decomposed {len(decomposed_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict 9574\n",
      "{'ideographic': 1840, 'pictographic': 227, 'pictophonetic': 6966, 'none': 541}\n",
      "\n",
      "example\n",
      "{'character': '⺊', 'pinyin': [], 'decomposition': '⿰丨？', 'etymology': {'type': 'ideographic', 'hint': 'A crack on an oracle bone; compare 卜'}, 'radical': '⺊', 'matches': [[0], None], 'definition': None}\n"
     ]
    }
   ],
   "source": [
    "# 'character', 'definition', 'pinyin', 'decomposition', 'etymology', 'radical', 'matches'\n",
    "# 'character', 'definition', 'pinyin', 'decomposition',              'radical', 'matches'\n",
    "# 'character',               'pinyin', 'decomposition', 'etymology', 'radical', 'matches'\n",
    "# 'character',               'pinyin', 'decomposition',              'radical', 'matches'\n",
    "\n",
    "dict_list = []\n",
    "keys = ('character', 'definition', 'pinyin', 'decomposition', 'etymology', 'radical', 'matches')\n",
    "with open(dic_dir) as f:\n",
    "    for row in f:\n",
    "        char_info = json.loads(row)\n",
    "        for key in keys:\n",
    "            try:\n",
    "                char_info[key]\n",
    "            except:\n",
    "                char_info[key] = None\n",
    "        dict_list.append(char_info)\n",
    "\n",
    "print(f'dict {len(dict_list)}')\n",
    "\n",
    "type_dict = {}\n",
    "\n",
    "for d in dict_list:\n",
    "    if d['etymology'] is not None:\n",
    "        type_name = d['etymology']['type']\n",
    "        if type_name in type_dict:\n",
    "            type_dict[type_name] += 1\n",
    "        else:\n",
    "            type_dict[type_name] = 1\n",
    "\n",
    "type_dict['none'] = len(dict_list) - sum(type_dict.values())\n",
    "\n",
    "print(type_dict)\n",
    "\n",
    "print(\"\\nexample\")\n",
    "print(dict_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideo_dict_list   = (d for d in dict_list if d['etymology'] is not None and d['etymology']['type'] == 'ideographic')\n",
    "picto_dict_list  = (d for d in dict_list if d['etymology'] is not None and d['etymology']['type'] == 'pictographic')\n",
    "picpho_dict_list = (d for d in dict_list if d['etymology'] is not None and d['etymology']['type'] == 'pictophonetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ideographic 1840\n",
      "pictographic 226\n",
      "pictophonetic 6966\n",
      "none 0\n"
     ]
    }
   ],
   "source": [
    "for type in type_dict:\n",
    "    count = 0\n",
    "    for d in (d for d in dict_list if d['etymology'] is not None and d['etymology']['type'] == type):\n",
    "        try:\n",
    "            decomposed_dict[hex(ord(d['character']))]\n",
    "            count += 1\n",
    "        except:\n",
    "            pass\n",
    "    print(f'{type} {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "sentences = {}\n",
    "for type in type_dict:\n",
    "    if type == 'none':\n",
    "        continue\n",
    "    decomposed = {}\n",
    "    for d in (d for d in dict_list if d['etymology'] is not None and d['etymology']['type'] == type):\n",
    "        try:\n",
    "            decomposed[hex(ord(d['character']))] = [hex(ord(c)) for c in decomposed_dict[hex(ord(d['character']))]]\n",
    "        except:\n",
    "            pass\n",
    "    sentences[type] = decomposed\n",
    "\n",
    "for k in sentences.keys():\n",
    "    chars = sentences[k]\n",
    "    uniques = set(reduce(lambda a, b: a+b, sentences[k].values()))\n",
    "\n",
    "    chars_dict = {}\n",
    "    end = 0\n",
    "    for i, c in enumerate(chars.keys(), start=1):\n",
    "        chars_dict[c] = i\n",
    "        end = i\n",
    "\n",
    "    uniques_dict = {}\n",
    "    for i, v in enumerate(uniques, start=1):\n",
    "        uniques_dict[v] = end + i\n",
    "\n",
    "\n",
    "    f = open(f\"radical-{k}.net\", \"w\")\n",
    "    f.write(f\"*Vertices {len(uniques)+len(chars)} {len(chars)}\\n\")\n",
    "    for c, i in chars_dict.items():\n",
    "        f.write(f\"{i}\\t\\\"{c}\\\"\\tic\\tRGBF92672\\tbc\\tRGB777777\\n\")\n",
    "    for c, i in uniques_dict.items():\n",
    "        f.write(f\"{i}\\t\\\"{c}\\\"\\tic\\tRGB66D9EF\\tbc\\tRGB777777\\n\")\n",
    "    f.write(\"*Arcs\\n*Edges\\n\")\n",
    "    for c, ids in chars.items():\n",
    "        for h in ids: \n",
    "            try:\n",
    "                f.write(f\"{chars_dict[c]}\\t{uniques_dict[h]}\\t1\\n\")\n",
    "            except:\n",
    "                print(f\"ERROR {h}\")\n",
    "    f.close()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9763644aba4be5567e39bc4edfabd7b2b04bb5e95bf3d0e4c310b07fda7d4e40"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('anaconda3-2021.05': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
